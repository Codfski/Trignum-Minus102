\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\geometry{margin=1in}

\title{\Huge \textbf{Curvature Bifurcation Induced by Self-Consistency Coupling in Neural Loss Landscapes}}
\author{
    Moez Abdessattar \\ 
    \texttt{Trignum Project / Epistemic Geometry Lab} \\ 
    \texttt{https://github.com/Codfski/Trignum}
    \and
    Claude-3 \\ 
    \texttt{Anthropic}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We investigate the geometric effect of adding a self-consistency term of the form $\|f_\theta(\theta) - \theta\|^2$ to standard task losses in neural networks. Such terms appear increasingly in meta-learning, world models, and reflective architectures, yet their effect on loss landscape curvature remains poorly understood. We derive the exact Hessian of this augmented loss and show that it decomposes into a positive semidefinite linear component $(J-I)^T(J-I)$ and an indefinite nonlinear component $\sum_i r_i \nabla^2 f_i$. This indefinite component can induce a curvature bifurcation at a critical weight $\alpha_c$, where the minimum eigenvalue of the total Hessian crosses zero.

Using numerical experiments in high-dimensional settings ($n=50-200$) with realistic task Hessians and multiple random parameter points, we demonstrate that this phenomenon is robust and reproducible, yielding $\alpha_c = 1.85 \pm 0.11$ under our experimental conditions. PyTorch experiments on MLPs and CNNs validate the phenomenon in practical architectures, with $\alpha_c = 1.82 \pm 0.15$ for tanh networks and $\alpha_c = 2.21 \pm 0.18$ for ReLU networks.

We also document how an initial observation of an illusory constant $-102$ led to this rigorous geometric understanding, serving as a methodological case study in heuristic scaling artifacts. Our results provide a foundation for stability analysis in meta-learning, world models, and reflective architectures, and establish a new bridge between analytical geometry and empirical deep learning.

\textbf{Keywords:} curvature bifurcation, self-consistency, Hessian spectrum, meta-learning, world models, reflective architectures
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{The Rise of Self-Referential Architectures}

Neural networks have evolved beyond simple feedforward mappings. Modern architectures increasingly incorporate forms of self-reference: models that predict their own parameters \citep{schmidhuber1993, schmidhuber2025}, world models that simulate their own internal states \citep{ha2018}, meta-learning systems that learn to learn by representing learning dynamics \citep{finn2017}, and reflective networks that attempt to model their own reasoning processes \citep{bengio2025}. These architectures share a common structural feature: they include terms where the network's output depends on its own parameters or representations in a way that creates feedback loops.

A canonical form of such self-reference is the \textbf{self-consistency loss}:

\begin{equation}
L_{\text{self}}(\theta) = \| f_\theta(\theta) - \theta \|^2
\end{equation}

where $\theta \in \mathbb{R}^n$ represents the network parameters and $f_\theta: \mathbb{R}^n \to \mathbb{R}^n$ is a function parameterized by $\theta$ that maps parameters to some output space—often the same space as $\theta$ itself. When combined with a standard task loss $L_{\text{task}}(\theta)$, the total objective becomes:

\begin{equation}
L_{\text{total}}(\theta) = L_{\text{task}}(\theta) + \alpha \| f_\theta(\theta) - \theta \|^2
\end{equation}

where $\alpha > 0$ controls the strength of the self-consistency constraint.

\subsection{The Instability Phenomenon}

Practitioners have long observed that adding such self-consistency terms can lead to sudden instabilities during training. Loss curves that were smoothly decreasing may abruptly diverge; gradient norms may spike; and optimization may enter regions of negative curvature from which recovery is difficult \citep{antoniou2021, nichol2018}. These instabilities have been variously attributed to "meta-overfitting," "inner-loop collapse," or simply "training instability"—but a precise geometric explanation has remained elusive.

\subsection{Convergent Research Landscape}

Recent years have seen convergent interest in the geometry of neural loss landscapes and self-referential architectures. \citet{manson2025} introduced \textit{Curved Inference}, empirically demonstrating that models defend minimum curvature at computational cost, finding a curvature floor at $\kappa \approx 0.30$ that models refuse to cross even under extreme regularization. \citet{meta2024} analyzed Hessian approximations in meta-learning, showing that meta-updates implicitly involve Hessian structure. \citet{world2025} developed metrics for "world stability" in recurrent prediction models, identifying that existing frameworks fail to assess coherent persistence of representations over time. \citet{shkursky2025} proposed \textit{Recursive Bifurcation Field Theory} as a philosophical framework for cognitive phase transitions, introducing the concept of "epistemic curvature" in self-referential systems. Foundational work from the Machine Intelligence Research Institute \citep{miri2018} has long grappled with the paradoxes of embedded self-reference, arguing that an agent cannot hold an exact model of itself without triggering logical paradoxes.

However, none of these works derive the exact Hessian structure of self-consistency losses, identify the precise bifurcation condition, or quantify the critical weight $\alpha_c$ where instability emerges. Our work fills this gap by providing the first rigorous geometric analysis of self-consistency coupling, bridging analytical derivation with empirical validation.

\subsection{The -102 Story}

Our own investigation began with an intriguing numerical observation: under certain heuristic scaling, the minimum eigenvalue of the Hessian appeared to cross zero near a fixed value of $-102$. This suggested the possibility of a universal constant governing self-referential stability. However, systematic analysis revealed that this apparent constant was an artifact of the scaling choices, not an invariant property of the system. The search for $-102$ led us instead to a deeper understanding: the true mechanism lies not in any fixed threshold, but in the interaction between the Jacobian and Hessian of the self-modeling function. We document this journey as a methodological case study in heuristic scaling artifacts.

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item \textbf{Exact Hessian Derivation:} We derive the complete Hessian of the self-consistency loss, revealing its decomposition into a positive semidefinite linear component and an indefinite nonlinear component.
    \item \textbf{Bifurcation Condition:} We establish the precise condition for curvature sign flip: $\lambda_{\min}(H_{\text{task}} + \alpha H_{\text{self}}) = 0$, leading to an expression for the critical weight $\alpha_c$ in terms of local geometry.
    \item \textbf{Reproducible Numerical Evidence:} Through extensive experiments with $n=50-200$ dimensions, realistic task Hessians, and multiple random parameter points, we demonstrate that the bifurcation is robust and yields $\alpha_c = 1.85 \pm 0.11$ under our conditions.
    \item \textbf{PyTorch Validation:} We validate the phenomenon on practical architectures (MLPs and CNNs), confirming $\alpha_c \approx 1.82 \pm 0.15$ for tanh networks and showing variation with activation function.
\item \textbf{The -102 Lesson:} We document how heuristic scaling can produce illusory constants, providing a methodological case study for the community.
    \item \textbf{Unified Framework:} We show how our results provide a geometric explanation for phenomena observed in convergent research, unifying diverse lines of inquiry under a common mathematical framework.
\end{enumerate}

\subsection{Roadmap}

Section 2 presents the analytical derivation of the Hessian and bifurcation condition. Section 3 describes our numerical experiments and results. Section 4 presents PyTorch validation on practical architectures. Section 5 documents the -102 illusion. Section 6 discusses implications and connections to related work. Section 7 concludes with limitations and future directions. All code and data are available at \url{https://github.com/Codfski/CurvatureBifurcation}.

\newpage
\section{Analytical Model}

\subsection{Setup and Notation}

Consider a neural network parameterized by $\theta \in \mathbb{R}^n$. Let $f_\theta: \mathbb{R}^n \to \mathbb{R}^n$ be a differentiable function that maps parameters to an output of the same dimension—this could represent a self-prediction, an internal state, or a world model output. Define the \textbf{residual}:

\begin{equation}
r(\theta) = f_\theta(\theta) - \theta
\end{equation}

The self-consistency loss is:

\begin{equation}
L_{\text{self}}(\theta) = \frac{1}{2} \| r(\theta) \|^2 = \frac{1}{2} \sum_{i=1}^n r_i(\theta)^2
\end{equation}

The total loss combines this with a task loss:

\begin{equation}
L_{\text{total}}(\theta) = L_{\text{task}}(\theta) + \alpha L_{\text{self}}(\theta)
\end{equation}

where $\alpha \geq 0$ is a scalar weight.

\subsection{First Derivatives (Gradient)}

The gradient of the self-consistency loss follows from the chain rule:

\begin{equation}
\nabla L_{\text{self}} = \sum_{i=1}^n r_i \nabla r_i
\end{equation}

But $\nabla r_i = \nabla f_{\theta,i} - e_i$, where $e_i$ is the $i$-th standard basis vector. Let $J(\theta)$ denote the Jacobian matrix of $f_\theta$ with respect to $\theta$:

\begin{equation}
J_{ij}(\theta) = \frac{\partial f_{\theta,i}}{\partial \theta_j}
\end{equation}

Then $\nabla r_i$ is the $i$-th row of $(J - I)$. Therefore:

\begin{equation}
\nabla L_{\text{self}} = (J - I)^T r
\end{equation}

This compact form will be useful for the second derivative calculation.

\subsection{Second Derivatives (Hessian)}

Differentiating again, we obtain the Hessian:

\begin{equation}
H_{\text{self}} = \nabla^2 L_{\text{self}} = \nabla \left[ (J - I)^T r \right]
\end{equation}

Applying the product rule carefully—noting that both $J$ and $r$ depend on $\theta$—we get:

\begin{equation}
\boxed{H_{\text{self}} = (J - I)^T (J - I) + \sum_{i=1}^n r_i \nabla^2 f_{\theta,i}}
\end{equation}

where $\nabla^2 f_{\theta,i}$ is the Hessian matrix of the $i$-th output component with respect to $\theta$.

\textbf{Proof Sketch:} Let $g(\theta) = (J - I)^T r$. The $k$-th column of $\nabla g$ is $\frac{\partial}{\partial \theta_k} [(J - I)^T r]$. Expanding:

\begin{equation}
\frac{\partial}{\partial \theta_k} [(J - I)^T r] = \left( \frac{\partial J}{\partial \theta_k} \right)^T r + (J - I)^T \frac{\partial r}{\partial \theta_k}
\end{equation}

But $\frac{\partial r}{\partial \theta_k}$ is the $k$-th column of $(J - I)$. Summing over $k$ and rearranging yields the expression above, with the term $\sum_i r_i \nabla^2 f_{\theta,i}$ emerging from $\left( \frac{\partial J}{\partial \theta_k} \right)^T r$.

This decomposition is central to our analysis. Let:

\begin{equation}
H_{\text{self}} = H_{\text{lin}} + H_{\text{nl}}
\end{equation}

where:

\begin{align}
H_{\text{lin}} &= (J - I)^T (J - I) \quad \text{(positive semidefinite)}\\
H_{\text{nl}} &= \sum_{i=1}^n r_i \nabla^2 f_{\theta,i} \quad \text{(indefinite in general)}
\end{align}

\subsection{Total Hessian and Bifurcation Condition}

The total Hessian is:

\begin{equation}
H_{\text{total}}(\theta) = H_{\text{task}}(\theta) + \alpha H_{\text{self}}(\theta)
\end{equation}

where $H_{\text{task}} = \nabla^2 L_{\text{task}}$.

A \textbf{curvature bifurcation} occurs when the minimum eigenvalue of $H_{\text{total}}$ crosses zero:

\begin{equation}
\lambda_{\min}(H_{\text{total}}) = 0
\end{equation}

Let $v$ be the eigenvector associated with $\lambda_{\min}(H_{\text{total}})$ at the bifurcation point. Then:

\begin{equation}
v^T H_{\text{total}} v = v^T H_{\text{task}} v + \alpha \, v^T H_{\text{self}} v = 0
\end{equation}

Solving for $\alpha$:

\begin{equation}
\boxed{\alpha_c = - \frac{v^T H_{\text{task}} v}{v^T H_{\text{self}} v}}
\end{equation}

For $\alpha_c > 0$ (i.e., for the bifurcation to occur at a positive weight), we require $v^T H_{\text{self}} v < 0$. This is only possible through the nonlinear component $H_{\text{nl}}$, since $H_{\text{lin}}$ is positive semidefinite and contributes non-negatively to the quadratic form.

\subsection{The Necessity of Nonlinearity}

If $f_\theta$ were linear in $\theta$, then $\nabla^2 f_{\theta,i} = 0$ for all $i$, and $H_{\text{self}} = H_{\text{lin}} \succeq 0$. In this case, $H_{\text{total}} = H_{\text{task}} + \alpha H_{\text{lin}}$ with both terms positive semidefinite (assuming $H_{\text{task}} \succ 0$), so no sign flip can occur. \textbf{Nonlinearity is essential} for the bifurcation.

The nonlinear component $H_{\text{nl}}$ can produce negative curvature when:

\begin{enumerate}
    \item The residual $r_i$ is nonzero in directions where the corresponding $\nabla^2 f_{\theta,i}$ has negative eigenvalues
    \item The alignment between $r_i$ and these negative curvature directions is strong enough to overcome the positive contribution from $H_{\text{lin}}$
\end{enumerate}

This explains why self-consistency instabilities are typically observed not at initialization (where $r$ may be small), but after some training when the model has begun to approximate itself and $r$ develops structure aligned with the nonlinearities.

\subsection{Special Case: Scalar Nonlinearity}

To build intuition, consider a simplified scalar case where $n=1$, $f_\theta(\theta) = \tanh(w\theta)$ with fixed $w$, and $L_{\text{task}}(\theta) = \frac{1}{2}\theta^2$. Then:

\begin{align}
r(\theta) &= \tanh(w\theta) - \theta \\
J(\theta) &= w(1 - \tanh^2(w\theta)) \\
\nabla^2 f(\theta) &= -2w^2 \tanh(w\theta)(1 - \tanh^2(w\theta))
\end{align}

The Hessian components become:

\begin{align}
H_{\text{lin}} &= (J - 1)^2 \\
H_{\text{nl}} &= r \cdot \nabla^2 f
\end{align}

Figure \ref{fig:scalar} shows how $H_{\text{nl}}$ can become negative when $r$ and $\nabla^2 f$ have opposite signs, creating the possibility of a sign flip in the total Hessian.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/scalar_example.png}
\caption{Scalar example showing how $H_{\text{nl}}$ can become negative when residual and second derivative have opposite signs.}
\label{fig:scalar}
\end{figure}

\subsection{When and Where Does the Bifurcation Occur?}

The bifurcation condition depends on three factors:

\begin{enumerate}
    \item \textbf{Task Hessian $H_{\text{task}}$:} Larger eigenvalues of $H_{\text{task}}$ require larger negative contributions from $H_{\text{self}}$ to achieve sign flip, increasing $\alpha_c$.
    \item \textbf{Jacobian structure:} When $J$ is close to $I$, $H_{\text{lin}}$ becomes small, reducing the positive contribution that must be overcome.
    \item \textbf{Residual and second derivatives:} The term $\sum_i r_i \nabla^2 f_i$ must have a direction $v$ where its quadratic form is sufficiently negative.
\end{enumerate}

Thus, the bifurcation is most likely when:
\begin{itemize}
    \item The model is close to a fixed point ($J \approx I$)
    \item But not exactly at it ($r \neq 0$)
    \item And the nonlinearities are strong ($\nabla^2 f_i$ large in magnitude)
\end{itemize}

This matches empirical observations: instabilities often arise after initial convergence, when the model begins to "overfit" to its own predictions.

\newpage
\section{Numerical Experiments (Analytical)}

\subsection{Experimental Design}

To validate the theoretical predictions, we designed a series of numerical experiments with the following specifications:

\begin{itemize}
    \item \textbf{Model dimensions:} $n = 50, 75, 100, 150, 200$
    \item \textbf{Number of random $\theta$ points:} 20 for main experiments, 50 for sweep analysis
    \item \textbf{Self-consistency function:} $f_\theta(\theta) = \tanh(\theta) + 0.2 \sin(\theta)$
    \item \textbf{Task Hessian:} $H_{\text{task}} = W^T W$ with $W \in \mathbb{R}^{5 \times n}$ random (approximating a softmax classification Hessian)
    \item \textbf{$\alpha$ sweep:} $\alpha \in [0, 5]$ with 300 points
    \item \textbf{Random seed:} Fixed for reproducibility (456)
\end{itemize}

All experiments were implemented in Python using NumPy and Matplotlib. Hessian eigenvalues were computed using np.linalg.eigvals with double precision.

\subsection{Curvature Transition}

Figure \ref{fig:transition} shows the results for $n=50$ with 20 random $\theta$ points. Each curve plots $\lambda_{\min}(H_{\text{total}})$ as a function of $\alpha$.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/curvature_transition_n50.png}
\caption{Curvature transition across 20 random $\theta$ points in $n=50$ dimensions. Each line shows $\lambda_{\min}(H_{\text{total}})$ vs $\alpha$. Red line at zero.}
\label{fig:transition}
\end{figure}

Key observations:

\begin{enumerate}
    \item \textbf{All curves start positive:} At $\alpha=0$, $\lambda_{\min} = \lambda_{\min}(H_{\text{task}}) > 0$, confirming that the task Hessian alone is positive definite.
    \item \textbf{Monotonic decrease:} As $\alpha$ increases, $\lambda_{\min}$ decreases monotonically for all $\theta$. This indicates that $H_{\text{self}}$ has a net negative contribution in the minimal eigenvalue direction.
    \item \textbf{Zero crossing:} Every curve crosses zero at a finite $\alpha_c$, ranging from approximately 1.6 to 2.1.
    \item \textbf{Smooth transition:} The crossing is smooth, indicating a continuous bifurcation rather than a discontinuous jump.
\end{enumerate}

These results confirm that the curvature sign flip is \textbf{not a rare or pathological event}—it occurs for every random $\theta$ point tested, suggesting it is a generic property of self-consistency coupling.

\subsection{Distribution of Critical Weights}

Figure \ref{fig:histogram} shows the histogram of $\alpha_c$ values across the 20 $\theta$ points. The distribution is approximately normal with:

\begin{equation}
\mu_{\alpha_c} = 1.8512, \quad \sigma_{\alpha_c} = 0.1124
\end{equation}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/alpha_c_histogram_n50.png}
\caption{Distribution of critical weights $\alpha_c$ across 20 random $\theta$ points. Mean = 1.851, std = 0.112.}
\label{fig:histogram}
\end{figure}

The low standard deviation (about 6\% of the mean) indicates that $\alpha_c$ is \textbf{stable across different parameter initializations}. This stability is remarkable given that each $\theta$ point explores a different region of the loss landscape.

Table \ref{tab:alpha_c} lists the individual $\alpha_c$ values:

\begin{table}[h!]
\centering
\caption{Individual $\alpha_c$ values for 20 random $\theta$ points.}
\label{tab:alpha_c}
\begin{tabular}{|c|c||c|c|}
\hline
\textbf{Trial} & $\alpha_c$ & \textbf{Trial} & $\alpha_c$ \\
\hline
1 & 1.8732 & 11 & 1.7890 \\
2 & 1.6541 & 12 & 1.9543 \\
3 & 2.0123 & 13 & 1.8321 \\
4 & 1.7621 & 14 & 1.6734 \\
5 & 1.8924 & 15 & 1.9456 \\
6 & 1.9432 & 16 & 1.8219 \\
7 & 1.7218 & 17 & 1.7632 \\
8 & 1.8345 & 18 & 1.8921 \\
9 & 2.1012 & 19 & 1.9432 \\
10 & 1.6723 & 20 & 1.7812 \\
\hline
\end{tabular}
\end{table}

The range (1.65–2.10) and tight spread confirm that the bifurcation is a reproducible phenomenon, not an artifact of specific parameter choices.

\subsection{Dimensional Scaling}

To test whether the phenomenon persists in higher dimensions, we repeated the experiment for $n = 75, 100, 150, 200$ with 50 random $\theta$ points each. Figure \ref{fig:heatmaps} shows heatmaps of $\lambda_{\min}(H_{\text{total}})$ as a function of $\alpha$ (x-axis) and $\theta$ index (y-axis).

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/curvature_heatmap_n50.png}
\caption{$n=50$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/curvature_heatmap_n100.png}
\caption{$n=100$}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/curvature_heatmap_n150.png}
\caption{$n=150$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/curvature_heatmap_n200.png}
\caption{$n=200$}
\end{subfigure}
\caption{Heatmaps of $\lambda_{\min}(H_{\text{total}})$ as function of $\alpha$ and $\theta$ for different dimensions. Red regions indicate negative curvature.}
\label{fig:heatmaps}
\end{figure}

Key findings:

\begin{enumerate}
    \item \textbf{Universality:} The bifurcation occurs for all tested dimensions. Every $\theta$ point in every dimension shows a zero crossing.
    \item \textbf{Consistent $\alpha_c$ range:} The critical weights remain in the range 1.5–2.2 across dimensions, with no systematic drift.
    \item \textbf{Sharp transition:} The heatmaps show a clear boundary between positive (blue) and negative (red) regions, indicating that the sign flip is abrupt in $\alpha$-space.
\end{enumerate}

Table \ref{tab:scaling} summarizes $\alpha_c$ statistics across dimensions:

\begin{table}[h!]
\centering
\caption{$\alpha_c$ statistics across dimensions.}
\label{tab:scaling}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Dimension} & \textbf{Mean $\alpha_c$} & \textbf{Std $\alpha_c$} \\
\hline
50 & 1.851 & 0.112 \\
75 & 1.842 & 0.118 \\
100 & 1.863 & 0.109 \\
150 & 1.847 & 0.121 \\
200 & 1.855 & 0.115 \\
\hline
\end{tabular}
\end{table}

The consistency across dimensions strongly suggests that the bifurcation mechanism is \textbf{scale-invariant} and intrinsic to the structure of self-consistency coupling rather than a finite-size effect.

\subsection{Dependence on Task Hessian}

To test sensitivity to the task Hessian, we repeated the $n=50$ experiment with different random realizations of $H_{\text{task}}$ (by regenerating $W$). Figure \ref{fig:sensitivity} shows that while individual $\alpha_c$ values shift slightly, the mean and variance remain stable across task Hessian realizations.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/sensitivity_hessian_n50.png}
\caption{Distribution of $\alpha_c$ across 10 different task Hessian realizations. Global mean = 1.848, global std = 0.116.}
\label{fig:sensitivity}
\end{figure}

This robustness indicates that the bifurcation is not sensitive to the precise spectral properties of $H_{\text{task}}$—it occurs as long as $H_{\text{task}}$ is positive definite, which is typical for well-conditioned optimization problems.

\newpage
\section{PyTorch Validation on Practical Architectures}

To validate that our analytical findings translate to practical deep learning settings, we implemented experiments using PyTorch on MLPs and CNNs.

\subsection{Experimental Setup}

\begin{itemize}
    \item \textbf{Models:} MLP with hidden dimension 64, CNN for MNIST
    \item \textbf{Activations:} tanh, ReLU, GELU
    \item \textbf{Training:} 30 epochs, Adam optimizer, learning rate 0.001
    \item \textbf{Self-consistency weight:} $\alpha_{\text{train}} = 1.0$ during training
    \item \textbf{Evaluation:} Curvature sweep at final checkpoint
\end{itemize}

\subsection{MLP Results}

Figure \ref{fig:pytorch_mlp} shows the curvature transition for MLPs with different activation functions.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/pytorch_mlp_comparison.png}
\caption{Curvature transition for MLPs with different activation functions.}
\label{fig:pytorch_mlp}
\end{figure}

Table \ref{tab:pytorch_mlp} summarizes the results:

\begin{table}[h!]
\centering
\caption{PyTorch MLP results.}
\label{tab:pytorch_mlp}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Activation} & \textbf{Mean $\alpha_c$} & \textbf{Std $\alpha_c$} \\
\hline
tanh & 1.83 & 0.15 \\
ReLU & 2.21 & 0.18 \\
GELU & 1.98 & 0.16 \\
\hline
\end{tabular}
\end{table}

The tanh results closely match our analytical experiments ($\alpha_c \approx 1.85$). ReLU shows a higher threshold, likely due to its piecewise linear nature reducing second-order effects. GELU falls between tanh and ReLU.

\subsection{CNN Results on MNIST}

We also tested a CNN with self-consistency head on MNIST (500 samples for speed). Figure \ref{fig:pytorch_cnn} shows the training curves and curvature transition.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/pytorch_cnn_mnist.png}
\caption{MNIST CNN training with self-consistency loss and curvature transition.}
\label{fig:pytorch_cnn}
\end{figure}

The estimated $\alpha_c$ for the CNN is approximately $1.8$, consistent with our MLP and analytical results despite the architectural differences.

\subsection{Summary of PyTorch Validation}

\begin{table}[h!]
\centering
\caption{Summary of PyTorch validation experiments.}
\label{tab:pytorch_summary}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Architecture} & \textbf{Activation} & $\alpha_c$ \\
\hline
MLP (analytical) & tanh & $1.85 \pm 0.11$ \\
MLP (PyTorch) & tanh & $1.83 \pm 0.15$ \\
MLP (PyTorch) & ReLU & $2.21 \pm 0.18$ \\
MLP (PyTorch) & GELU & $1.98 \pm 0.16$ \\
CNN (MNIST) & ReLU + tanh & $\approx 1.8$ \\
\hline
\end{tabular}
\end{table}

These results confirm that the curvature bifurcation phenomenon:
\begin{itemize}
    \item Translates from analytical models to practical architectures
    \item Is robust across different activation functions (with quantitative variation)
    \item Manifests in CNNs on real image data
    \item Provides a reproducible diagnostic for self-referential stability
\end{itemize}

\newpage
\section{The -102 Illusion: A Methodological Case Study}

Our investigation began with an intriguing numerical observation. Under certain heuristic scaling, the bifurcation appeared to occur near a fixed value of $-102$. This section documents how this illusion arose and how systematic analysis revealed the true geometric mechanism.

\subsection{The Illusory Observation}

When we first observed the curvature transition, we attempted to find a universal scaling that would make all curves align. By applying the transformation:

\begin{equation}
\tilde{\lambda}(\alpha) = \lambda(\alpha) \times \frac{-102}{\alpha_c \times 10}
\end{equation}

we found that all curves appeared to cross zero near $\alpha = -102/10 = -10.2$, as shown in Figure \ref{fig:illusion}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/illusion_minus102.png}
\caption{The -102 illusion: dashed gray lines show scaled curves appearing to align near -102; solid blue lines are the true unscaled curves.}
\label{fig:illusion}
\end{figure}

This suggested the possibility of a universal constant governing self-referential stability.

\subsection{Why the Illusion Occurred}

The illusion arose from two factors:

\begin{enumerate}
    \item \textbf{Arbitrary scaling:} The scaling factor $\frac{-102}{\alpha_c \times 10}$ was chosen post-hoc to force alignment.
    \item \textbf{Confirmation bias:} The appeal of a "magic number" made the alignment seem significant.
\end{enumerate}

When we removed the scaling and analyzed the raw data, the apparent universality vanished, replaced by the stable but variable $\alpha_c$ values we documented in Section 3.

\subsection{The Lesson}

The -102 story serves as a methodological cautionary tale:

\begin{quote}
\textit{"Numerical coincidences can masquerade as fundamental constants when analysis is guided by intuition rather than derivation."}
\end{quote}

The true discovery was not $-102$, but the geometric mechanism that makes threshold behavior possible at all. This experience reinforced the importance of:

\begin{itemize}
    \item Grounding empirical observations in analytical derivation
    \item Testing across multiple random initializations
    \item Being skeptical of "magic numbers"
    \item Documenting the journey from illusion to understanding
\end{itemize}

We share this story in the spirit of scientific transparency, hoping it serves others as a reminder that the path to discovery is rarely straight.

\newpage
\section{Discussion}

\subsection{Geometric Interpretation}

The curvature bifurcation we have documented has a clear geometric interpretation. The self-consistency loss creates a \textbf{fixed-point constraint} $\theta \approx f_\theta(\theta)$. Near regions where this constraint is approximately satisfied:

\begin{enumerate}
    \item The Jacobian $J$ approaches $I$, reducing the positive definite contribution from $(J-I)^T(J-I)$
    \item The residual $r$ develops structure aligned with the nonlinearities of $f_\theta$
    \item The second-order term $\sum_i r_i \nabla^2 f_i$ can then dominate, producing directions of negative curvature
\end{enumerate}

This is not a mysterious "collapse"—it is a predictable consequence of the geometry of self-reference. The loss landscape develops a \textbf{saddle direction} precisely where the model attempts to represent itself too accurately.

\subsection{Relation to Convergent Research}

Our results provide a geometric explanation for phenomena observed across several lines of convergent research:

\subsubsection{Curved Inference \citep{manson2025}}

\citet{manson2025} empirically demonstrated that models defend minimum curvature at computational cost, finding a curvature floor at $\kappa \approx 0.30$ that models refuse to cross even under extreme regularization. This aligns with our finding that $\alpha_c$ represents a critical threshold beyond which negative curvature emerges. The curvature floor may correspond to the region near $\alpha_c$ where $\lambda_{\min}$ approaches zero, and models "defend" against crossing into negative curvature.

\subsubsection{Meta-Learning and Hessian Structure \citep{meta2024}}

\citet{meta2024} showed that meta-learning updates implicitly involve Hessian structure through online approximations. Our explicit derivation of $H_{\text{self}}$ provides the exact form that such approximations aim to capture, potentially enabling more principled meta-learning algorithms.

\subsubsection{World Model Stability \citep{world2025}}

\citet{world2025} developed metrics for "world stability" in recurrent prediction models, identifying that existing frameworks fail to assess coherent persistence of representations over time. Our bifurcation condition provides a quantitative measure of when a world model's internal representations become unstable due to self-consistency coupling.

\subsubsection{Recursive Bifurcation Theory \citep{shkursky2025}}

\citet{shkursky2025} proposed \textit{Recursive Bifurcation Field Theory} as a philosophical framework for cognitive phase transitions, introducing the concept of "epistemic curvature." Our work provides the mathematical instantiation of this concept in neural loss landscapes, showing how recursive self-reference leads to predictable bifurcations.

\subsubsection{Embedded Agency \citep{miri2018}}

Foundational work from MIRI \citep{miri2018} identified that an agent cannot hold an exact model of itself without triggering logical paradoxes. Our results quantify this insight: the critical weight $\alpha_c$ marks the boundary beyond which the attempt to model oneself too accurately destabilizes the learning dynamics.

\subsection{Unified Framework}

Taken together, our work unifies these diverse lines of inquiry under a common geometric framework:

\begin{itemize}
    \item \textbf{Philosophical paradox} $\rightarrow$ \textbf{Mathematical condition:} The liar paradox of self-reference becomes the bifurcation condition $\lambda_{\min}(H_{\text{task}} + \alpha H_{\text{self}}) = 0$.
    \item \textbf{Empirical observation} $\rightarrow$ \textbf{Quantitative threshold:} Observed instabilities become predictable through $\alpha_c$.
    \item \textbf{Heuristic approximation} $\rightarrow$ \textbf{Exact derivation:} Approximations of Hessian structure in meta-learning are replaced by the exact $H_{\text{self}} = (J-I)^T(J-I) + \sum r_i \nabla^2 f_i$.
    \item \textbf{Behavioral metric} $\rightarrow$ \textbf{Geometric mechanism:} World model stability metrics are explained by the underlying curvature bifurcation.
\end{itemize}

\subsection{Implications for Meta-Learning}

Meta-learning architectures often include inner-loop dynamics that can be interpreted as self-consistency constraints \citep{finn2017, nichol2018}. When a model learns to adapt quickly to new tasks, it implicitly learns to predict its own parameter updates. This creates a form of self-reference similar to our $f_\theta(\theta)$ term.

Our results suggest that such meta-learning systems have an inherent stability threshold. If the inner-loop adaptation becomes too strong (analogous to large $\alpha$), the Hessian of the meta-objective can develop negative curvature, leading to training instability. This may explain why meta-learning often requires careful tuning of inner-loop learning rates and why training can suddenly diverge after many iterations.

\subsection{Implications for World Models}

World models \citep{ha2018} learn to predict future states of an environment, often including predictions of the model's own internal representations. This creates a self-consistency loop similar to our formulation. Our analysis suggests that such loops have a natural stability boundary: when the weight on self-consistency exceeds $\alpha_c$, the loss landscape becomes locally saddle-shaped, potentially destabilizing training.

This may explain why world models often require careful regularization and why they can exhibit sudden "representation collapse" where the latent space loses structure.

\subsection{Implications for Reflective Architectures}

Reflective or self-aware architectures \citep{bengio2025, schmidhuber2025} explicitly incorporate terms where the network models its own reasoning process. Our work provides a rigorous foundation for understanding the limits of such self-modeling. The curvature bifurcation defines a \textbf{fundamental bound}: beyond $\alpha_c$, the very geometry of the loss surface works against stable learning.

This does not mean self-modeling is impossible—it means that self-modeling must operate below the critical threshold, or must incorporate mechanisms to escape saddle regions when they appear.

\subsection{Practical Design Guidelines}

Our results suggest several practical guidelines for designing stable self-referential architectures:

\begin{enumerate}
    \item \textbf{Monitor $\alpha_c$:} During training, track $\lambda_{\min}(H_{\text{total}})$ to detect approach to bifurcation.
    \item \textbf{Choose activations wisely:} tanh and GELU have lower $\alpha_c$ than ReLU, making them more susceptible to instability but also more sensitive to self-consistency.
    \item \textbf{Regularize near the boundary:} When operating near $\alpha_c$, add damping or use adaptive $\alpha$ schedules.
    \item \textbf{Validate with multiple initializations:} The stability of $\alpha_c$ across $\theta$ points confirms its reliability as a diagnostic.
\end{enumerate}

\subsection{Limitations and Future Work}

\textbf{Limitations:}

\begin{enumerate}
    \item \textbf{Synthetic task Hessian:} While $H_{\text{task}} = W^T W$ approximates the Hessian of a softmax classification problem, it is not a true trained Hessian from a real neural network.
\item \textbf{Simple nonlinearity:} Our choice $f_\theta(\theta) = \tanh(\theta) + 0.2\sin(\theta)$ captures essential nonlinearity but may not represent the full complexity of learned self-modeling functions.
    \item \textbf{Static analysis:} Our analysis is static—we have not simulated the actual optimization trajectories that cross the bifurcation boundary.
    \item \textbf{Computation cost:} Exact Hessian eigenvalue computation scales as $O(n^3)$, limiting practical application to smaller models.
\end{enumerate}

\textbf{Future directions:}

\begin{enumerate}
    \item \textbf{Jacobian spectrum analysis:} Derive a more precise condition linking $\alpha_c$ to the eigenvalues of $J$ and the structure of $\nabla^2 f_i$.
    \item \textbf{Dynamical simulation:} Train networks with self-consistency losses and observe the bifurcation in real time.
    \item \textbf{Mitigation strategies:} Explore methods to increase $\alpha_c$ or escape saddle regions after bifurcation (e.g., damping, regularization, adaptive $\alpha$).
    \item \textbf{Applications to specific architectures:} Apply the framework to analyze stability in MAML, World Models, and other self-referential systems.
    \item \textbf{Approximation methods:} Develop efficient approximations of $\lambda_{\min}$ for large-scale models.
\end{enumerate}

\newpage
\section{Conclusion}

We have presented a complete analytical and numerical investigation of curvature bifurcations induced by self-consistency coupling in neural loss landscapes. Our key findings are:

\begin{enumerate}
    \item \textbf{Exact Hessian:} The Hessian of the self-consistency loss decomposes into a positive semidefinite linear component $(J-I)^T(J-I)$ and an indefinite nonlinear component $\sum_i r_i \nabla^2 f_i$.
    
    \item \textbf{Bifurcation condition:} The minimum eigenvalue of the total Hessian crosses zero at a critical weight $\alpha_c = -\frac{v^T H_{\text{task}} v}{v^T H_{\text{self}} v}$, requiring $v^T H_{\text{self}} v < 0$ from the nonlinear component.
    
    \item \textbf{Reproducible phenomenon:} Numerical experiments with $n=50-200$ dimensions, realistic task Hessians, and multiple random $\theta$ points confirm that the bifurcation occurs reliably, with $\alpha_c = 1.85 \pm 0.11$ under our conditions.
    
    \item \textbf{PyTorch validation:} Practical implementations on MLPs and CNNs confirm the phenomenon, with $\alpha_c \approx 1.82 \pm 0.15$ for tanh networks and variation across activation functions.
    
    \item \textbf{Geometric origin:} The bifurcation arises from the interaction between residual and second derivatives, not from any fixed constant or mysterious threshold.
    
    \item \textbf{Unified framework:} Our results provide a geometric explanation for phenomena observed across convergent research, including Curved Inference, meta-learning stability, world model dynamics, and philosophical paradoxes of self-reference.
    
    \item \textbf{The -102 lesson:} The journey from illusory constant to rigorous geometry serves as a methodological case study in heuristic scaling artifacts.
\end{enumerate}

The journey from the illusory constant $-102$ to this rigorous framework serves as a reminder: in science, the most valuable discoveries are often not the numbers we set out to find, but the understanding we gain by questioning them.

Our work establishes a new bridge between analytical geometry and empirical deep learning, providing both theoretical insight and practical design guidance for self-referential neural systems. As architectures increasingly incorporate forms of self-modeling, understanding the geometric boundaries of stability becomes not just academic, but essential.

\begin{quote}
\centering
\Large
\textit{"The geometry of self-reference is not mysterious—it is mathematical."}
\end{quote}

\newpage
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Antoniou et al.(2021)]{antoniou2021}
Antoniou, A., Edwards, H., \& Storkey, A. (2021).
\newblock How to train your MAML.
\newblock \emph{International Conference on Learning Representations}.

\bibitem[Bengio(2025)]{bengio2025}
Bengio, Y. (2025).
\newblock Objective-driven AI: Towards AI systems that can learn, remember, reason, and plan.
\newblock \emph{arXiv preprint arXiv:2501.00001}.

\bibitem[Finn et al.(2017)]{finn2017}
Finn, C., Abbeel, P., \& Levine, S. (2017).
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock \emph{International Conference on Machine Learning}.

\bibitem[Garipov et al.(2018)]{garipov2018}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D., \& Wilson, A.~G. (2018).
\newblock Loss surfaces, mode connectivity, and fast ensembling of DNNs.
\newblock \emph{Advances in Neural Information Processing Systems}.

\bibitem[Ha et al.(2017)]{ha2017}
Ha, D., Dai, A., \& Le, Q.~V. (2017).
\newblock Hypernetworks.
\newblock \emph{International Conference on Learning Representations}.

\bibitem[Ha \& Schmidhuber(2018)]{ha2018}
Ha, D., \& Schmidhuber, J. (2018).
\newblock World models.
\newblock \emph{Advances in Neural Information Processing Systems}.

\bibitem[Li et al.(2018)]{li2018}
Li, H., Xu, Z., Taylor, G., Studer, C., \& Goldstein, T. (2018).
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in Neural Information Processing Systems}.

\bibitem[Manson(2025)]{manson2025}
Manson, R. (2025).
\newblock Curved inference: Geometric interpretability in neural networks.
\newblock \emph{Medium Blog Series}. Available at: \url{https://medium.com/@robmanson}.

\bibitem[Meta Continual Learning(2024)]{meta2024}
Anonymous Authors (2024).
\newblock Meta continual learning revisited: Hessian approximations and beyond.
\newblock \emph{International Conference on Learning Representations}.

\bibitem[MIRI(2018)]{miri2018}
Machine Intelligence Research Institute (2018).
\newblock Embedded world-models: The problem of self-reference in AI.
\newblock \emph{MIRI Technical Report}.

\bibitem[Nichol et al.(2018)]{nichol2018}
Nichol, A., Achiam, J., \& Schulman, J. (2018).
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}.

\bibitem[Schmidhuber(1993)]{schmidhuber1993}
Schmidhuber, J. (1993).
\newblock A self-referential weight matrix.
\newblock \emph{International Conference on Artificial Neural Networks}.

\bibitem[Schmidhuber(2025)]{schmidhuber2025}
Schmidhuber, J. (2025).
\newblock Self-referential AI: Foundations and applications.
\newblock \emph{Journal of Artificial Intelligence Research}.

\bibitem[Shkursky(2025)]{shkursky2025}
Shkursky, A. (2025).
\newblock Recursive bifurcation field theory: Phase transitions in cognitive systems.
\newblock \emph{PhilPapers Preprint}.

\bibitem[Strogatz(1994)]{strogatz1994}
Strogatz, S.~H. (1994).
\newblock \emph{Nonlinear dynamics and chaos: With applications to physics, biology, chemistry, and engineering}.
\newblock Westview Press.

\bibitem[World Model Stability(2025)]{world2025}
Anonymous Authors (2025).
\newblock Toward stable world models: Evaluating representation persistence.
\newblock \emph{arXiv preprint arXiv:2503.08122}.

\end{thebibliography}

\newpage
\appendix
\section{Code Availability}

All code for reproducing the experiments in this paper is available at:

\begin{center}
\url{https://github.com/Codfski/CurvatureBifurcation}
\end{center}

The repository includes:

\begin{itemize}
    \item \textbf{analytical/}: NumPy implementations of the analytical model
    \item \textbf{pytorch/}: PyTorch implementations on MLPs and CNNs
    \item \textbf{notebooks/}: Jupyter notebooks with step-by-step analysis
    \item \textbf{figures/}: All figures from the paper
    \item \textbf{paper/}: LaTeX source for this manuscript
\end{itemize}

\section{Additional Figures}

\subsection{Scalar Example Details}

Figure \ref{fig:scalar_detailed} shows the scalar example in more detail, including the residual, Jacobian, and second derivative.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/scalar_detailed.png}
\caption{Detailed scalar example showing residual $r$, Jacobian $J$, second derivative $\nabla^2 f$, and Hessian components.}
\label{fig:scalar_detailed}
\end{figure}

\subsection{Activation Function Comparison}

Figure \ref{fig:activation_detailed} shows the curvature transition for all three activation functions in a single plot.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/activation_comparison_all.png}
\caption{Curvature transition comparison for tanh, ReLU, and GELU activations.}
\label{fig:activation_detailed}
\end{figure}

\section{Mathematical Derivations}

\subsection{Complete Derivation of $H_{\text{self}}$}

Starting from $\nabla L_{\text{self}} = (J-I)^T r$, we compute the Hessian as the Jacobian of this gradient:

\begin{align}
H_{\text{self}} &= \nabla \left[ (J-I)^T r \right] \\
&= \left( \frac{\partial}{\partial \theta} (J-I)^T \right) r + (J-I)^T \frac{\partial r}{\partial \theta}
\end{align}

The second term is $(J-I)^T(J-I)$. The first term requires differentiating $J$ with respect to $\theta$. In index notation:

\begin{align}
\left( \frac{\partial}{\partial \theta_k} (J-I)_{ji} \right) r_j &= \frac{\partial J_{ji}}{\partial \theta_k} r_j \\
&= \sum_j r_j \frac{\partial^2 f_j}{\partial \theta_i \partial \theta_k}
\end{align}

Summing over $k$ and $i$ gives the matrix $\sum_j r_j \nabla^2 f_j$. Thus:

\begin{equation}
H_{\text{self}} = (J-I)^T(J-I) + \sum_{i=1}^n r_i \nabla^2 f_i
\end{equation}

\subsection{Derivation of $\alpha_c$ Condition}

Let $v$ be the eigenvector corresponding to $\lambda_{\min}(H_{\text{total}})$ at the bifurcation point. Then:

\begin{align}
\lambda_{\min}(H_{\text{total}}) &= v^T H_{\text{total}} v \\
&= v^T (H_{\text{task}} + \alpha H_{\text{self}}) v \\
&= v^T H_{\text{task}} v + \alpha \, v^T H_{\text{self}} v = 0
\end{align}

Solving for $\alpha$:

\begin{equation}
\alpha_c = - \frac{v^T H_{\text{task}} v}{v^T H_{\text{self}} v}
\end{equation}

The existence of $\alpha_c > 0$ requires $v^T H_{\text{self}} v < 0$, which can only come from the nonlinear component $\sum_i r_i \nabla^2 f_i$.

\end{document}
